{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Critique-Revise\n",
    "\n",
    "This is an basic example of correcting an LLM's output using a pattern called critique-revise, where we highlight what part of the output is wrong and re-query the LLM for a correction.\n",
    "\n",
    "In the below example, we define a Pydantic schema to match an array of tasks we want the LLM to compose for a given input. Take note that we expect the output tasks to match one of a defined set of types, and to return a valid ISO string as a due date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install openai\n",
    "\n",
    "from typing import Sequence\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "class TaskType(str, Enum):\n",
    "    call = \"Call\"\n",
    "    message = \"Message\"\n",
    "    todo = \"Todo\"\n",
    "    in_person_meeting = \"In-Person Meeting\"\n",
    "    email = \"Email\"\n",
    "    mail = \"Mail\"\n",
    "    text = \"Text\"\n",
    "    open_house = \"Open House\"\n",
    "\n",
    "class Task(BaseModel):\n",
    "    title: str = Field(..., description=\"The title of the tasks, reminders and alerts\")\n",
    "    due_date: datetime = Field(..., description=\"Due date. Must be a valid ISO date string with timezone\")\n",
    "    task_type: TaskType = Field(None, description=\"The type of task\")\n",
    "    \n",
    "class Tasks(BaseModel):\n",
    "    \"\"\"JSON definition for creating tasks, reminders and alerts\"\"\"\n",
    "    tasks: Sequence[Task]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this Pydantic schema with an OpenAI model as a function, and invoke it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions import (\n",
    "    convert_to_openai_function\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "\n",
    "template = \"\"\"Respond to the following user query to the best of your ability:\n",
    "\n",
    "{query}\"\"\";\n",
    "\n",
    "generate_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "user_query = \"Set a reminder to renew our online property ads next week.\"\n",
    "\n",
    "function_args = {\"functions\": [convert_to_openai_function(Tasks)]}\n",
    "\n",
    "task_function_call_model = ChatOpenAI(model=\"gpt-3.5-turbo\").bind(**function_args)\n",
    "\n",
    "output_parser = JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tasks': [{'title': 'Renew online property ads', 'due_date': 'next week', 'task_type': 'Reminder'}]}\n"
     ]
    }
   ],
   "source": [
    "# const traceGroup = new TraceGroup(\"CritiqueReviseChain\");\n",
    "# const groupManager = await traceGroup.start();\n",
    "\n",
    "chain = generate_prompt | task_function_call_model | output_parser\n",
    "\n",
    "result = chain.invoke({ \"query\": user_query })\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that despite the passed function schema, the model was influenced by the particular user input, and the the outputted task type does not match one of the defined types. We use our original Pydantic model's `validate` method in a wrapped validator method to show this. We extract the error message summarizing the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " \"2 validation errors for Tasks\\ntasks -> 0 -> due_date\\n  invalid datetime format (type=value_error.datetime)\\ntasks -> 0 -> task_type\\n  value is not a valid enumeration member; permitted: 'Call', 'Message', 'Todo', 'In-Person Meeting', 'Email', 'Mail', 'Text', 'Open House' (type=type_error.enum; enum_values=[<TaskType.call: 'Call'>, <TaskType.message: 'Message'>, <TaskType.todo: 'Todo'>, <TaskType.in_person_meeting: 'In-Person Meeting'>, <TaskType.email: 'Email'>, <TaskType.mail: 'Mail'>, <TaskType.text: 'Text'>, <TaskType.open_house: 'Open House'>])\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def output_validator(output):\n",
    "    try:\n",
    "        Tasks.validate(output)\n",
    "    except ValidationError as e:\n",
    "        return (False, str(e))\n",
    "    \n",
    "    return [True]\n",
    "\n",
    "output_validator(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a revise chain that will attempt to fix the problem. We reuse the previously defined model with the bound function call arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "revise_template = \"\"\"Original prompt:\n",
    "--------------\n",
    "{original_prompt}\n",
    "--------------\n",
    "\n",
    "Completion:\n",
    "--------------\n",
    "{completion}\n",
    "--------------\n",
    "\n",
    "Above, the completion did not satisfy the constraints given by the original prompt and provided schema.\n",
    "\n",
    "Error:\n",
    "--------------\n",
    "{error}\n",
    "--------------\n",
    "\n",
    "Try again. Only respond with an answer that satisfies the constraints laid out in the original prompt and provided schema:\"\"\"\n",
    "\n",
    "revise_prompt = ChatPromptTemplate.from_template(revise_template)\n",
    "\n",
    "revise_chain = (revise_prompt | task_function_call_model | JsonOutputFunctionsParser()).with_config(run_name=\"ReviseChain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we rerun the erroneous output with the formatted original prompt, completion, and error message until it passes the validation successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tasks': [{'title': 'Renew online property ads', 'due_date': '2022-12-12', 'task_type': 'Reminder'}]}\n",
      "{'tasks': [{'title': 'Renew online property ads', 'due_date': '2022-12-12', 'task_type': 'Reminder'}]}\n",
      "{'tasks': [{'title': 'Renew online property ads', 'due_date': '2022-12-12', 'task_type': 'Todo'}]}\n",
      "{'tasks': [{'title': 'Renew online property ads', 'due_date': '2022-12-12T12:00:00Z', 'task_type': 'Todo'}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import (\n",
    "    trace_as_chain_group,\n",
    ")\n",
    "import json\n",
    "\n",
    "with trace_as_chain_group(\"CritiqueRevise\", inputs={\"query\": user_query}) as group_manager:\n",
    "    fix_count = 0\n",
    "    formatted_original_prompt = generate_prompt.format(query=user_query)\n",
    "    result = chain.invoke({ \"query\": user_query }, config={\"callbacks\": group_manager})\n",
    "    validator_result = output_validator(result)\n",
    "    while validator_result[0] == False and fix_count < 5:\n",
    "        result = revise_chain.invoke({\n",
    "            \"original_prompt\": formatted_original_prompt,\n",
    "            \"completion\": json.dumps(result),\n",
    "            \"error\": json.dumps(validator_result[1])\n",
    "        }, config={\"callbacks\": group_manager})\n",
    "        print(result)\n",
    "        validator_result = output_validator(result)\n",
    "        fix_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the final result matches our original Pydantic schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': [{'title': 'Renew online property ads',\n",
       "   'due_date': '2022-12-12T12:00:00Z',\n",
       "   'task_type': 'Todo'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can peruse the following LangSmith trace showing it in action:\n",
    "\n",
    "https://smith.langchain.com/public/7e554bfa-2ed8-4d10-a06a-396c82f22820/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
